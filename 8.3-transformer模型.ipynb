{"cells":[{"outputs":[],"execution_count":5,"source":"import os\nimport math\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\n# sys.path.append('/home/kesci/input/d2len9900')\n# import d2l","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4672830ED4AF481381BC61AF99945B22","scrolled":false}},{"metadata":{"id":"05E44331F0524D108E0095F909DE8209","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 上节的mask softmax操作\ndef SequenceMask(X, X_len,value=-1e6):\n    maxlen = X.size(1)\n    X_len = X_len.to(X.device)\n    #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] )\n    mask = torch.arange((maxlen), dtype=torch.float, device=X.device)\n    mask = mask[None, :] < X_len[:, None]\n    #print(mask)\n    X[~mask]=value\n    return X\n\ndef masked_softmax(X, valid_length):\n    # X: 3-D tensor, valid_length: 1-D or 2-D tensor\n    softmax = nn.Softmax(dim=-1)\n    if valid_length is None:\n        return softmax(X)\n    else:\n        shape = X.shape\n        if valid_length.dim() == 1:\n            try:\n                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n            except:\n                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n        else:\n            valid_length = valid_length.reshape((-1,))\n        # fill masked elements with a large negative, whose exp is 0\n        X = SequenceMask(X.reshape((-1, shape[-1])), valid_length)\n \n        return softmax(X).reshape(shape)\n\n# Save to the d2l package.\nclass DotProductAttention(nn.Module): \n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # query: (batch_size, #queries, d)\n    # key: (batch_size, #kv_pairs, d)\n    # value: (batch_size, #kv_pairs, dim_v)\n    # valid_length: either (batch_size, ) or (batch_size, xx)\n    def forward(self, query, key, value, valid_length=None):\n        d = query.shape[-1]\n        # set transpose_b=True to swap the last two dimensions of key\n        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        return torch.bmm(attention_weights, value)","execution_count":6},{"metadata":{"id":"805F4D7FAFC4414493841FD896606B1C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# Q K V 三者维度调换函数\ndef transpose_qkv(X, num_heads):\n    # Original X shape: (batch_size, seq_len, hidden_size * num_heads),\n    # -1 means inferring its value, after first reshape, X shape:\n    # (batch_size, seq_len, num_heads, hidden_size)\n    X = X.view(X.shape[0], X.shape[1], num_heads, -1)\n    \n    # After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)\n    X = X.transpose(2, 1).contiguous()\n\n    # Merge the first two dimensions. Use reverse=True to infer shape from\n    # right to left.\n    # output shape: (batch_size * num_heads, seq_len, hidden_size)\n    output = X.view(-1, X.shape[2], X.shape[3])\n    return output\n    \n# 用于多头注意力层输出的维度调换\ndef transpose_output(X, num_heads):\n    # A reversed version of transpose_qkv\n    X = X.view(-1, num_heads, X.shape[1], X.shape[2])\n    X = X.transpose(2, 1).contiguous()\n    return X.view(X.shape[0], X.shape[1], -1)\n# 多头注意力层\n# 假设我们有h个头，隐藏层权重hidden_size=Pq=Pk=Pv与query，key，value的维度一致\n# 除此之外，因为多头注意力层保持输入与输出张量的维度不变，所以输出feature的维度也设置为hidden_size 。\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, input_size, hidden_size, num_heads, dropout, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.attention = DotProductAttention(dropout)  # 点积注意力\n        self.W_q = nn.Linear(input_size, hidden_size, bias=False)  # Q\n        self.W_k = nn.Linear(input_size, hidden_size, bias=False)  # K\n        self.W_v = nn.Linear(input_size, hidden_size, bias=False)  # V\n        self.W_o = nn.Linear(hidden_size, hidden_size, bias=False)  # O\n    \n    def forward(self, query, key, value, valid_length):\n        # query, key, and value shape: (batch_size, seq_len, dim),\n        # where seq_len is the length of input sequence\n        # valid_length shape is either (batch_size, )\n        # or (batch_size, seq_len).\n\n        # Project and transpose query, key, and value from\n        # (batch_size, seq_len, hidden_size * num_heads) to\n        # (batch_size * num_heads, seq_len, hidden_size).\n    \n        query = transpose_qkv(self.W_q(query), self.num_heads)  # 进行维度变化\n        key = transpose_qkv(self.W_k(key), self.num_heads)\n        value = transpose_qkv(self.W_v(value), self.num_heads)\n        \n        if valid_length is not None:  # 将有效长度进行编码（即向量化）\n            # Copy valid_length by num_heads times\n            device = valid_length.device\n            valid_length = valid_length.cpu().numpy() if valid_length.is_cuda else valid_length.numpy()\n            if valid_length.ndim == 1:\n                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))\n            else:\n                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,1)))\n\n            valid_length = valid_length.to(device)\n            \n        output = self.attention(query, key, value, valid_length)  # 计算点积注意力值\n        output_concat = transpose_output(output, self.num_heads)  # 所有自注意力拼接成需求的shape\n        output_  = self.W_o(output_concat)  # 最后进行全连接层映射\n        return output_  ","execution_count":7},{"metadata":{"id":"75800FC9C5714D139B2952B09AFA6651","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([2, 4, 9])"},"transient":{},"execution_count":8}],"source":"# 看下效果\ncell = MultiHeadAttention(5, 9, 3, 0.5)\nX = torch.ones((2, 4, 5))\nvalid_length = torch.FloatTensor([2, 3])\ncell(X, X, X, valid_length).shape","execution_count":8},{"metadata":{"id":"283415C1D5004D8A8F63C26EBBBB4CF0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# Transformer 模块另一个非常重要的部分就是基于位置的前馈网络（FFN），它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。\n# 简单的全连接网络，对每个 position 的向量分别进行相同的操作，包括两个线性变换和一个 ReLU 激活输出\nclass PositionWiseFFN(nn.Module):\n    def __init__(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs):\n        super(PositionWiseFFN, self).__init__(**kwargs)\n        self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)\n        self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)\n        \n        \n    def forward(self, X):\n        return self.ffn_2(F.relu(self.ffn_1(X)))\n# 与多头注意力层相似，FFN层同样只会对最后一维的大小进行改变；\n# 除此之外，对于两个完全相同的输入，FFN层的输出也将相等。","execution_count":9},{"metadata":{"id":"CAFA9EABB1D34C1E816550DECCA8E6DB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[[ 0.1496,  0.2851,  0.0109, -0.0132, -0.2647,  0.1160, -0.2861,\n           0.7412],\n         [ 0.1496,  0.2851,  0.0109, -0.0132, -0.2647,  0.1160, -0.2861,\n           0.7412],\n         [ 0.1496,  0.2851,  0.0109, -0.0132, -0.2647,  0.1160, -0.2861,\n           0.7412]],\n\n        [[ 0.1496,  0.2851,  0.0109, -0.0132, -0.2647,  0.1160, -0.2861,\n           0.7412],\n         [ 0.1496,  0.2851,  0.0109, -0.0132, -0.2647,  0.1160, -0.2861,\n           0.7412],\n         [ 0.1496,  0.2851,  0.0109, -0.0132, -0.2647,  0.1160, -0.2861,\n           0.7412]]], grad_fn=<AddBackward0>) torch.Size([2, 3, 8])\n","name":"stdout"}],"source":"# 查看效果\nffn = PositionWiseFFN(4, 4, 8)\nout = ffn(torch.ones((2,3,4)))\n\nprint(out, out.shape)","execution_count":10},{"metadata":{"id":"6361253ACE1B43E59AF55933897C457E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 除了上面两个模块之外，Transformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出\n# 因此我们在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。\n\n# # 对比layernorm和BN，加深理解   ？？\n# layernorm = nn.LayerNorm(normalized_shape=2, elementwise_affine=True)\n# batchnorm = nn.BatchNorm1d(num_features=2, affine=True)\n# X = torch.FloatTensor([[1,2], [3,4]])\n# print('layer norm:', layernorm(X))\n# print('batch norm:', batchnorm(X))\n\n# 实现相加归一化层\nclass AddNorm(nn.Module):\n    def __init__(self, hidden_size, dropout, **kwargs):\n        super(AddNorm, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.norm = nn.LayerNorm(hidden_size)\n    \n    def forward(self, X, Y):\n        # 易知此处涉及矩阵相加，因此要求X Y的shape相同\n        return self.norm(self.dropout(Y) + X)","execution_count":11},{"metadata":{"id":"363DC9D1BD8D4D3FAA8B3CC5DAE1C02B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([2, 3, 4])"},"transient":{},"execution_count":12}],"source":"# 查看想家归一化层的效果\nadd_norm = AddNorm(4, 0.5)\nadd_norm(torch.ones((2,3,4)), torch.ones((2,3,4))).shape","execution_count":12},{"metadata":{"id":"E63618CE662C4AD38452391E97B91408","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 位置编码网络层\n# 为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。\nclass PositionalEncoding(nn.Module):\n    def __init__(self, embedding_size, dropout, max_len=1000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.P = np.zeros((1, max_len, embedding_size))\n        X = np.arange(0, max_len).reshape(-1, 1) / np.power(\n            10000, np.arange(0, embedding_size, 2)/embedding_size)\n        self.P[:, :, 0::2] = np.sin(X)\n        self.P[:, :, 1::2] = np.cos(X)\n        self.P = torch.FloatTensor(self.P)\n    \n    def forward(self, X):\n        if X.is_cuda and not self.P.is_cuda:\n            self.P = self.P.cuda()\n        X = X + self.P[:, :X.shape[1], :]\n        return self.dropout(X)","execution_count":13},{"metadata":{"id":"1F00AC3445BD4C9693718DD23F59C1CA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'd2l' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-a25d9f7412bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m d2l.plot(np.arange(100), Y[0, :, 4:8].T, figsize=(6, 2.5),\n\u001b[0m\u001b[1;32m      6\u001b[0m          legend=[\"dim %d\" % p for p in [4, 5, 6, 7]])\n","\u001b[0;31mNameError\u001b[0m: name 'd2l' is not defined"]}],"source":"# 查看PositionalEncoding效果\nimport numpy as np\npe = PositionalEncoding(20, 0)\nY = pe(torch.zeros((1, 100, 20))).numpy()\nd2l.plot(np.arange(100), Y[0, :, 4:8].T, figsize=(6, 2.5),\n         legend=[\"dim %d\" % p for p in [4, 5, 6, 7]])","execution_count":14},{"metadata":{"id":"0AE3D96C972F418D84FD79FA975DE976","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 开始搭建transformer编码器\n# 编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层\nclass EncoderBlock(nn.Module):\n    def __init__(self, embedding_size, ffn_hidden_size, num_heads,\n                 dropout, **kwargs):\n        super(EncoderBlock, self).__init__(**kwargs)\n        self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)\n        self.addnorm_1 = AddNorm(embedding_size, dropout)\n        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)\n        self.addnorm_2 = AddNorm(embedding_size, dropout)\n    \n    def forward(self, X, valid_length):\n        o1 = self.attention(X, X, X, valid_length)\n        o2 = self.addnorm_1(X, o1)\n        Y = self.addnorm_2(o2, self.ffn(o2))\n        # Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))\n        # return self.addnorm_2(Y, self.ffn(Y))\n        return Y","execution_count":15},{"metadata":{"id":"B5F0087EFE6D4819A41807967034D922","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([2, 100, 24])"},"transient":{},"execution_count":16}],"source":"# batch_size = 2, seq_len = 100, embedding_size = 24\n# ffn_hidden_size = 48, num_head = 8, dropout = 0.5\n# 查看下效果\nX = torch.ones((2, 100, 24))\nencoder_blk = EncoderBlock(24, 48, 8, 0.5)\nencoder_blk(X, valid_length).shape","execution_count":16},{"metadata":{"id":"E0B7C3F02DC746F6AB739603FC6E40E8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([2, 100, 24])"},"transient":{},"execution_count":17}],"source":"class Encoder(nn.Module):\n    def __init__(self, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n\n    def forward(self, X, *args):\n        raise NotImplementedError\n# 实现整个Transformer 编码器模型\nclass TransformerEncoder(Encoder):\n    def __init__(self, vocab_size, embedding_size, ffn_hidden_size,\n                 num_heads, num_layers, dropout, **kwargs):\n        super(TransformerEncoder, self).__init__(**kwargs)\n        self.embedding_size = embedding_size\n        self.embed = nn.Embedding(vocab_size, embedding_size)\n        self.pos_encoding = PositionalEncoding(embedding_size, dropout)\n        self.blks = nn.ModuleList()\n        for i in range(num_layers):\n            self.blks.append(\n                EncoderBlock(embedding_size, ffn_hidden_size,\n                             num_heads, dropout))\n\n    def forward(self, X, valid_length, *args):\n        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))\n        for blk in self.blks:\n            X = blk(X, valid_length)\n        return X\n\n# test encoder\nencoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\nencoder(torch.ones((2, 100)).long(), valid_length).shape","execution_count":17},{"metadata":{"id":"3AD97A849C9641B2BB56C53EF2B8BDBC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 搭建transformer解码器模块\nclass DecoderBlock(nn.Module):\n    def __init__(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs):\n        super(DecoderBlock, self).__init__(**kwargs)\n        self.i = i\n        self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)\n        self.addnorm_1 = AddNorm(embedding_size, dropout)\n        self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)\n        self.addnorm_2 = AddNorm(embedding_size, dropout)\n        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)\n        self.addnorm_3 = AddNorm(embedding_size, dropout)\n    \n    def forward(self, X, state):\n        enc_outputs, enc_valid_length = state[0], state[1]\n        \n        # state[2][self.i] stores all the previous t-1 query state of layer-i\n        # len(state[2]) = num_layers\n        \n        # If training:\n        #     state[2] is useless.\n        # If predicting:\n        #     In the t-th timestep:\n        #         state[2][self.i].shape = (batch_size, t-1, hidden_size)\n        # Demo:\n        # love dogs ! [EOS]\n        #  |    |   |   |\n        #   Transformer \n        #    Decoder\n        #  |   |   |   |\n        #  I love dogs !\n        \n        if state[2][self.i] is None:\n            key_values = X\n        else:\n            # shape of key_values = (batch_size, t, hidden_size)\n            key_values = torch.cat((state[2][self.i], X), dim=1) \n        state[2][self.i] = key_values\n        \n        if self.training:\n            batch_size, seq_len, _ = X.shape\n            # Shape: (batch_size, seq_len), the values in the j-th column are j+1\n            valid_length = torch.FloatTensor(np.tile(np.arange(1, seq_len+1), (batch_size, 1))) \n            valid_length = valid_length.to(X.device)\n        else:\n            valid_length = None\n\n        X2 = self.attention_1(X, key_values, key_values, valid_length)\n        Y = self.addnorm_1(X, X2)\n        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)\n        Z = self.addnorm_2(Y, Y2)\n        return self.addnorm_3(Z, self.ffn(Z)), state","execution_count":18},{"metadata":{"id":"B98FB0AC87B94E15804B28A3DA3EF0EB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([2, 100, 24])"},"transient":{},"execution_count":19}],"source":"# 查看解码器模块效果\ndecoder_blk = DecoderBlock(24, 48, 8, 0.5, 0)\nX = torch.ones((2, 100, 24))\nstate = [encoder_blk(X, valid_length), valid_length, [None]]\ndecoder_blk(X, state)[0].shape","execution_count":19},{"metadata":{"id":"03CEFBEFBC394D63883564F299EB9572","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class Decoder(nn.Module):\n    def __init__(self, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n\n    def init_state(self, enc_outputs, *args):\n        raise NotImplementedError\n\n    def forward(self, X, state):\n        raise NotImplementedError\n# Transformer Decoder整体实现\n# 除了常规的超参数例如vocab_size embedding_size 之\n# 解码器还需要编码器的输出 enc_outputs 和句子有效长度 enc_valid_length。\nclass TransformerDecoder(Decoder):\n    def __init__(self, vocab_size, embedding_size, ffn_hidden_size,\n                 num_heads, num_layers, dropout, **kwargs):\n        super(TransformerDecoder, self).__init__(**kwargs)\n        self.embedding_size = embedding_size\n        self.num_layers = num_layers\n        self.embed = nn.Embedding(vocab_size, embedding_size)\n        self.pos_encoding = PositionalEncoding(embedding_size, dropout)\n        self.blks = nn.ModuleList()\n        for i in range(num_layers):\n            self.blks.append(\n                DecoderBlock(embedding_size, ffn_hidden_size, num_heads,\n                             dropout, i))\n        self.dense = nn.Linear(embedding_size, vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_length, *args):\n        return [enc_outputs, enc_valid_length, [None]*self.num_layers]\n\n    def forward(self, X, state):\n        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))\n        for blk in self.blks:\n            X, state = blk(X, state)\n        return self.dense(X), state","execution_count":21},{"metadata":{"id":"C68A0FB38E37437B8C758058BE5A8D72","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 训练\nimport zipfile\nimport torch\nimport requests\nfrom io import BytesIO\nfrom torch.utils import data\nimport sys\nimport collections\n\nclass Vocab(object): # This class is saved in d2l.\n  def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n    # sort by frequency and token\n    counter = collections.Counter(tokens)\n    token_freqs = sorted(counter.items(), key=lambda x: x[0])\n    token_freqs.sort(key=lambda x: x[1], reverse=True)\n    if use_special_tokens:\n      # padding, begin of sentence, end of sentence, unknown\n      self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n      tokens = ['', '', '', '']\n    else:\n      self.unk = 0\n      tokens = ['']\n    tokens += [token for token, freq in token_freqs if freq >= min_freq]\n    self.idx_to_token = []\n    self.token_to_idx = dict()\n    for token in tokens:\n      self.idx_to_token.append(token)\n      self.token_to_idx[token] = len(self.idx_to_token) - 1\n      \n  def __len__(self):\n    return len(self.idx_to_token)\n  \n  def __getitem__(self, tokens):\n    if not isinstance(tokens, (list, tuple)):\n      return self.token_to_idx.get(tokens, self.unk)\n    else:\n      return [self.__getitem__(token) for token in tokens]\n    \n  def to_tokens(self, indices):\n    if not isinstance(indices, (list, tuple)):\n      return self.idx_to_token[indices]\n    else:\n      return [self.idx_to_token[index] for index in indices]\n\ndef load_data_nmt(batch_size, max_len, num_examples=1000):\n    \"\"\"Download an NMT dataset, return its vocabulary and data iterator.\"\"\"\n    # Download and preprocess\n    def preprocess_raw(text):\n        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n        out = ''\n        for i, char in enumerate(text.lower()):\n            if char in (',', '!', '.') and text[i-1] != ' ':\n                out += ' '\n            out += char\n        return out \n\n\n    with open('/home/kesci/input/fraeng6506/fra.txt', 'r') as f:\n      raw_text = f.read()\n\n\n    text = preprocess_raw(raw_text)\n\n    # Tokenize\n    source, target = [], []\n    for i, line in enumerate(text.split('\\n')):\n        if i >= num_examples:\n            break\n        parts = line.split('\\t')\n        if len(parts) >= 2:\n            source.append(parts[0].split(' '))\n            target.append(parts[1].split(' '))\n\n    # Build vocab\n    def build_vocab(tokens):\n        tokens = [token for line in tokens for token in line]\n        return Vocab(tokens, min_freq=3, use_special_tokens=True)\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n\n    # Convert to index arrays\n    def pad(line, max_len, padding_token):\n        if len(line) > max_len:\n            return line[:max_len]\n        return line + [padding_token] * (max_len - len(line))\n\n    def build_array(lines, vocab, max_len, is_source):\n        lines = [vocab[line] for line in lines]\n        if not is_source:\n            lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n        array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])\n        valid_len = (array != vocab.pad).sum(1)\n        return array, valid_len\n\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)\n    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)\n    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)\n    return src_vocab, tgt_vocab, train_iter","execution_count":22},{"metadata":{"id":"B90DA7E634114BBCB10DD2AAD1C7BCE6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'home'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-e98586132785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/kesci/input/d2len9900'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkesci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2len9900\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 平台暂时不支持gpu，现在会自动使用cpu训练，gpu可以用了之后会使用gpu来训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'home'"]}],"source":"import os\nsys.path.append('/home/kesci/input/d2len9900')\nimport home.kesci.input.d2len9900.d2l\n\n# 平台暂时不支持gpu，现在会自动使用cpu训练，gpu可以用了之后会使用gpu来训练\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nembed_size, embedding_size, num_layers, dropout = 32, 32, 2, 0.05\nbatch_size, num_steps = 64, 10\nlr, num_epochs, ctx = 0.005, 250, d2l.try_gpu()\nprint(ctx)\nnum_hiddens, num_heads = 64, 4\n\nsrc_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)\n\nencoder = TransformerEncoder(\n    len(src_vocab), embedding_size, num_hiddens, num_heads, num_layers,\n    dropout)\ndecoder = TransformerDecoder(\n    len(src_vocab), embedding_size, num_hiddens, num_heads, num_layers,\n    dropout)\n    \nmodel = d2l.EncoderDecoder(encoder, decoder)\nd2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)","execution_count":26},{"metadata":{"id":"C4ACF95F19884C4DBB27DE3CADE225FA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'input'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-11559283a0af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2len9900\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'input'"]}],"source":"","execution_count":39}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}