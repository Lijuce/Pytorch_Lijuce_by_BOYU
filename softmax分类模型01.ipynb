{"cells":[{"metadata":{"id":"0870A4A80E2045C28323177820901763","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nCollecting torchtext\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n\u001b[K     |████████████████████████████████| 81kB 3.3kB/s eta 0:00:01\n\u001b[?25hCollecting sentencepiece (from torchtext)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/e0/1264990c559fb945cfb6664742001608e1ed8359eeec6722830ae085062b/sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n\u001b[K     |████████████████████████████████| 1.0MB 99kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.17.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.12.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext) (4.32.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext) (2.22.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (1.25.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (2019.9.11)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (2.8)\nInstalling collected packages: sentencepiece, torchtext\nSuccessfully installed sentencepiece-0.1.85 torchtext-0.5.0\n1.3.0\n0.4.1a0+d94043a\n","name":"stdout"}],"source":"!pip install torchtext\nimport torch\nimport torchvision\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\nprint(torch.__version__)\nprint(torchvision.__version__)","execution_count":1},{"metadata":{"id":"57F56B7F9B374564B028E0FAC292035C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"batch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root='/home/kesci/input/FashionMNIST2065')","execution_count":2},{"metadata":{"id":"4CB45FC5B5EF41E18726E4923E343348","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 模型参数初始化\nnum_inputs = 784  # 28*28\nnum_outputs = 10\n\nW = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)\nb = torch.zeros(num_outputs, dtype=torch.float)","execution_count":3},{"metadata":{"id":"B2622193CB104D7F87BD811582143153","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"},"transient":{},"execution_count":4}],"source":"# 参数递归赋能\nW.requires_grad_(requires_grad=True)\nb.requires_grad_(requires_grad=True)","execution_count":4},{"metadata":{"id":"0FE33715A7DA419C99EB1F7B1EA7F414","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[5, 7, 9]])\ntensor([[ 6],\n        [15]])\ntensor([5, 7, 9])\ntensor([ 6, 15])\n","name":"stdout"}],"source":"# 对多维Tensor按不同维度操作\nX = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(X.sum(dim=0, keepdim=True))  # dim为0，按照相同的列求和，并在结果中保留列特征\nprint(X.sum(dim=1, keepdim=True))  # dim为1，按照相同的行求和，并在结果中保留行特征\nprint(X.sum(dim=0, keepdim=False)) # dim为0，按照相同的列求和，不在结果中保留列特征\nprint(X.sum(dim=1, keepdim=False)) # dim为1，按照相同的行求和，不在结果中保留行特征","execution_count":5},{"metadata":{"id":"DA0D4CDA54FE4BF5851991C1FB3C9B7B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 定义softmax数学模型\ndef softmax(X):\n    X_exp = X.exp()\n    partition = X_exp.sum(dim=1, keepdim=True)\n    return X_exp / partition  # 这里应用了广播机制","execution_count":6},{"metadata":{"id":"C5D28F7970234FAF876908D0437C7955","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[0.3252, 0.1876, 0.1627, 0.1424, 0.1821],\n        [0.1651, 0.1308, 0.2968, 0.1642, 0.2431]]) \n tensor([1., 1.])\n","name":"stdout"}],"source":"# 尝试上面定义的softmax模型\nX = torch.rand((2, 5))\nX_prob = softmax(X)\nprint(X_prob, '\\n', X_prob.sum(dim=1))","execution_count":7},{"metadata":{"id":"DC1C346D0A8248FE82A6746A9F3D9703","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 定义softmax回归模型\ndef net(X):\n    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)","execution_count":8},{"metadata":{"id":"CFCF7563216640F19AD3D281670B8AA6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 用来测试，便于下面定义交叉熵函数工程上的理解\n# y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n# y = torch.LongTensor([0, 2])\n# y_hat.gather(1, y.view(-1, 1))\n\n# 定义损失函数-交叉熵\ndef cross_entropy(y_hat, y):\n    return - torch.log(y_hat.gather(1, y.view(-1, 1)))","execution_count":9},{"metadata":{"id":"A939647F317C40F68B2AE4A7FC796321","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中描述\ndef evaluate_accuracy(data_iter, net):\n    acc_sum, n = 0.0, 0\n    for X, y in data_iter:\n        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n        n += y.shape[0]\n    return acc_sum / n","execution_count":10},{"metadata":{"id":"FC157FC05A874812B80B8CF82231406E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 训练模型\nnum_epochs, lr = 5, 0.1\n\n# 本函数已保存在d2lzh_pytorch包中方便以后使用\ndef train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n              params=None, lr=None, optimizer=None):\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n        for X, y in train_iter:\n            y_hat = net(X)\n            l = loss(y_hat, y).sum()\n            \n            # 梯度清零\n            if optimizer is not None:\n                optimizer.zero_grad()\n            elif params is not None and params[0].grad is not None:\n                for param in params:\n                    param.grad.data.zero_()\n            \n            l.backward()\n            if optimizer is None:\n                d2l.sgd(params, lr, batch_size)\n            else:\n                optimizer.step() \n            \n            \n            train_l_sum += l.item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n            n += y.shape[0]\n        test_acc = evaluate_accuracy(test_iter, net)\n        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n","execution_count":13},{"metadata":{"id":"F941AD7C3ACF4746898264FFC5435273","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 1, loss 0.7854, train acc 0.748, test acc 0.788\nepoch 2, loss 0.5708, train acc 0.812, test acc 0.806\nepoch 3, loss 0.5264, train acc 0.825, test acc 0.819\nepoch 4, loss 0.5005, train acc 0.832, test acc 0.825\nepoch 5, loss 0.4855, train acc 0.838, test acc 0.827\n","name":"stdout"}],"source":"train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)","execution_count":14}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}