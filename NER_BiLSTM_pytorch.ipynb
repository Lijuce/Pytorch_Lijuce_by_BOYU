{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import torchtext.vocab as Vocab\n",
    "import codecs\n",
    "import collections\n",
    "import torch.utils.data as Data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1、导入原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_sentence(data_path):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(data_path, 'r', 'utf8'):\n",
    "        line = line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2、制作数据词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vocab(sentences):    \n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        for sen in sentence:\n",
    "            word = sen[0]\n",
    "            words.append(word)\n",
    "    counter = collections.Counter(words)\n",
    "    vocab = Vocab.Vocab(counter)\n",
    "    return vocab, len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3、制作数据索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_index(sentences, vocab, tag_to_idx):\n",
    "    \n",
    "    max_l = max([len(sentence) for sentence in sentences])\n",
    "    def pad(x):  # 补全语料句子长度\n",
    "        return x + [0] * (max_l - len(x))\n",
    "    \n",
    "    tokenized_data = []\n",
    "    tags_sentence = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = []\n",
    "        sentence_tag = []\n",
    "        for w in sentence:\n",
    "            tokenized_sentence.append(vocab.stoi[w[0]])  # TEXT.vocab.stoi 对应词寻找下标； itos 对应下标寻找词 \n",
    "            sentence_tag.append(tag_to_idx[w[1]])\n",
    "        tokenized_data.append(pad(tokenized_sentence))\n",
    "        tags_sentence.append(pad(sentence_tag))\n",
    "    features = torch.tensor(tokenized_data, dtype=torch.long)\n",
    "    labels = torch.tensor(tags_sentence, dtype=torch.long)\n",
    "    return features, labels\n",
    "# features, labels = get_index(NER_sentences, NER_vocab, tag_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 4、导入词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n",
    "        pretrained_vocab: 预训练词向量\n",
    "    @return:\n",
    "        embed: 加载到的词向量\n",
    "    '''\n",
    "    embed= torch.zeros(len(words), 100)  # 初始化语料对应词向量\n",
    "    oov_count = 0  # 计总袋外词数量\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 5、搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size, hidden_dim, out_dim, BiFlag=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim * (2 if BiFlag else 1)\n",
    "        # 定义词嵌入层\n",
    "        self.embedding = nn.Embedding(input_dim, embed_size)\n",
    "        # 定义LSTM网络的输入、输出、是否双向\n",
    "        self.rnn_layer = nn.LSTM(input_size=embed_size, hidden_size=hidden_dim,\n",
    "                                bidirectional=BiFlag)\n",
    "        # 定义线性分类层\n",
    "        self.dense = nn.Linear(self.hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, inputs, state):  # 前向传播计算\n",
    "        # 嵌入层要求输入格式为longTensor\n",
    "        x = self.embedding(inputs)  # shape：(batch_size, vocab_size, embed_size)\n",
    "        hiddens, state = self.rnn_layer(x, state)  # hiddens为输出  state为最新的隐层状态\n",
    "        # 由于以上的输入x为3D的tensor，因此需要转化为2D的tensor\n",
    "        hiddens = hiddens.view(-1, hiddens.shape[-1])  # hiddens.shape: (num_steps * batch_size, hidden_size)\n",
    "        outputs = self.dense(hiddens)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 6、启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_NER(model, num_epochs, train_data):\n",
    "    loss = nn.CrossEntropyLoss()  # 在使用CrossEntropyLoss()这个函数进行验证时，标签必须从0开始设置，否则便会报错。\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    model.to(device)\n",
    "    state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum = []\n",
    "        d_iter = make_data_iter(NER_source, NER_target)\n",
    "        for x,y in train_data:\n",
    "            if state is not None:\n",
    "                if isinstance (state, tuple): # LSTM, state:(h, c)  \n",
    "                    state = (state[0].detach(), state[1].detach())\n",
    "                else:\n",
    "                    state = state.detach()\n",
    "            (output, state) = model(x, state)\n",
    "            l = loss(output, y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum.append(float(l))\n",
    "        print(\"epoch %d , loss %f\"%(epoch, np.mean(l_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D30D8BABBC824EBBBBD4CC8F20F982FC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tag_to_idx = {\"<PAD>\": 0, \"O\": 1, \"B-LOC\": 2, \"I-LOC\": 3, \"B-PER\":4, \"I-PER\":5, \"B-ORG\": 6, \"I-ORG\": 7}\n",
    "idx_to_tag = {\"0\":\"<PAD>\", \"1\": \"O\", \"2\": \"B-LOC\", \"3\": \"I-LOC\", \"4\": \"B-PER\", \"5\": \"I-PER\", \"6\":\"B-ORG\", \"7\":\"I-ORG\"}\n",
    "NER_sentences = load_sentence(\"./dataset/example.train\")\n",
    "NER_vocab, vocab_size= get_vocab(NER_sentences)  # 语料词典和词典大小\n",
    "tags_size = len(tag_to_idx)  # target标签大小\n",
    "NER_source, NER_target = get_index(NER_sentences, NER_vocab, tag_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CA5EE906951D483F82D14DEA40E5CA59",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 oov words.\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"./dataset\"\n",
    "glove_vocab = Vocab.Vectors(name='wiki_100.utf8', cache=cache_dir)\n",
    "vocab_vec = load_pretrained_embedding(NER_vocab.itos, glove_vocab)\n",
    "train_set  = Data.TensorDataset(NER_source[:1000], NER_target[:1000])\n",
    "train_iter = Data.DataLoader(train_set, 50, shuffle=True)\n",
    "model = RNNModel(vocab_size, 100, 128, tags_size)\n",
    "model.embedding.weight.data.copy_(vocab_vec)\n",
    "model.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CA5EE906951D483F82D14DEA40E5CA59",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_NER(model, 5, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "740C023FA78B45EFAF17949CB7FD4F0C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 574])\n",
      "tensor([[ 454,  585,  199,  ...,    0,    0,    0],\n",
      "        [  19,  347,   96,  ...,    0,    0,    0],\n",
      "        [ 784, 1192,  511,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  24,   38,  230,  ...,    0,    0,    0],\n",
      "        [1874,   95,  278,  ...,    0,    0,    0],\n",
      "        [  95,  185,   38,  ...,    0,    0,    0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set  = Data.TensorDataset(NER_source[:1000], NER_target[:1000])\n",
    "train_iter = Data.DataLoader(train_set, 50, shuffle=True)\n",
    "# for x,y in train_iter:\n",
    "#     print(x.shape)\n",
    "#     print(x)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
