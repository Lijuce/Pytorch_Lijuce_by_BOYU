{"cells":[{"outputs":[],"execution_count":1,"source":"import math\nimport torch \nimport torch.nn as nn","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A137051675C841158BF48BD6B3A3F4BE","scrolled":false}},{"metadata":{"id":"DD9ED3E631F34EFD8B496996C351FA98","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 由于机器翻译的可变长度，需进行屏蔽操作\ndef SequenceMask(X, X_len,value=-1e6):\n    maxlen = X.size(1)\n    #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] )\n    mask = torch.arange((maxlen),dtype=torch.float)[None, :] >= X_len[:, None]   \n    #print(mask)\n    X[mask]=value\n    return X\n\n# 有屏蔽作用的softmax运算\ndef masked_softmax(X, valid_length):\n    # X: 3-D tensor, valid_length: 1-D or 2-D tensor\n    softmax = nn.Softmax(dim=-1)\n    if valid_length is None:\n        return softmax(X)\n    else:\n        shape = X.shape\n        if valid_length.dim() == 1:\n            try:\n                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n            except:\n                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n        else:\n            valid_length = valid_length.reshape((-1,))\n        # fill masked elements with a large negative, whose exp is 0\n        X = SequenceMask(X.reshape((-1, shape[-1])), valid_length)\n \n        return softmax(X).reshape(shape)","execution_count":3},{"metadata":{"id":"29D6C77032634D5A84BB3B6FFA978D5A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[[0.3666, 0.6334, 0.0000, 0.0000],\n         [0.5222, 0.4778, 0.0000, 0.0000]],\n\n        [[0.4201, 0.2135, 0.3664, 0.0000],\n         [0.2314, 0.2622, 0.5064, 0.0000]]])"},"transient":{},"execution_count":4}],"source":"# 查看效果\nmasked_softmax(torch.rand((2,2,4),dtype=torch.float), torch.FloatTensor([2,3]))","execution_count":4},{"metadata":{"id":"F5F1F3F754A6406D8FCFEAD259E8EA91","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[[3., 3.]],\n\n        [[3., 3.]]])"},"transient":{},"execution_count":5}],"source":"# 超出二维的矩阵乘法运算\n# torch的bmm函数\ntorch.bmm(torch.ones((2,1,3), dtype = torch.float), torch.ones((2,3,2), dtype = torch.float))","execution_count":5},{"metadata":{"id":"37D2B1DAA2AD48128D4304EF90CE6319","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 点积注意力 操作\nclass DotProductAttention(nn.Module): \n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # query: (batch_size, #queries, d)\n    # key: (batch_size, #kv_pairs, d)\n    # value: (batch_size, #kv_pairs, dim_v)\n    # valid_length: either (batch_size, ) or (batch_size, xx)\n    def forward(self, query, key, value, valid_length=None):\n        d = query.shape[-1]\n        # set transpose_b=True to swap the last two dimensions of key\n        \n        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        print(\"attention_weight\\n\",attention_weights)\n        return torch.bmm(attention_weights, value)","execution_count":6},{"metadata":{"id":"7C0728E74F98431C817F17EECBDA8400","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"attention_weight\n tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000]],\n\n        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n          0.0000, 0.0000]]])\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n\n        [[10.0000, 11.0000, 12.0000, 13.0000]]])"},"transient":{},"execution_count":7}],"source":"# 查看效果\n# 创建了两个批，每个批有一个query和10个key-values对。\n# 我们通过valid_length指定，对于第一批，我们只关注前2个键-值对，\n# 而对于第二批，我们将检查前6个键-值对。\n# 因此，尽管这两个批处理具有相同的查询和键值对，但我们获得的输出是不同的。\natten = DotProductAttention(dropout=0)\n\nkeys = torch.ones((2,10,2),dtype=torch.float)\nvalues = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\natten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))","execution_count":7},{"metadata":{"id":"3461D624C9FC430C80F986B312AA660D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n\n        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward>)"},"transient":{},"execution_count":8}],"source":"# 多层感知注意力\nclass MLPAttention(nn.Module):  \n    def __init__(self, units,ipt_dim,dropout, **kwargs):\n        super(MLPAttention, self).__init__(**kwargs)\n        # Use flatten=True to keep query's and key's 3-D shapes.\n        self.W_k = nn.Linear(ipt_dim, units, bias=False)\n        self.W_q = nn.Linear(ipt_dim, units, bias=False)\n        self.v = nn.Linear(units, 1, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, valid_length):\n        query, key = self.W_k(query), self.W_q(key)\n        #print(\"size\",query.size(),key.size())\n        # expand query to (batch_size, #querys, 1, units), and key to\n        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.\n        features = query.unsqueeze(2) + key.unsqueeze(1)\n        #print(\"features:\",features.size())  #--------------开启\n        scores = self.v(features).squeeze(-1) \n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        return torch.bmm(attention_weights, value)\n\natten = MLPAttention(ipt_dim=2,units = 8, dropout=0)\natten(torch.ones((2,1,2), dtype = torch.float), keys, values, torch.FloatTensor([2, 6]))","execution_count":8},{"metadata":{"id":"E8A6BBEB63CB4068A729D6DEBB085392","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class Encoder(nn.Module):\n    def __init__(self, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n\n    def forward(self, X, *args):\n        raise NotImplementedError\n\nclass Decoder(nn.Module):\n    def __init__(self, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n\n    def init_state(self, enc_outputs, *args):\n        raise NotImplementedError\n\n    def forward(self, X, state):\n        raise NotImplementedError\n        \nclass EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(EncoderDecoder, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, enc_X, dec_X, *args):\n        enc_outputs = self.encoder(enc_X, *args)\n        dec_state = self.decoder.init_state(enc_outputs, *args)\n        return self.decoder(dec_X, dec_state)\n\nclass Seq2SeqEncoder(Encoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqEncoder, self).__init__(**kwargs)\n        self.num_hiddens = num_hiddens\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(vocab_size, embed_size)  #nn模块的词嵌入方法\n        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers, dropout=dropout)\n    \n    # 隐藏状态初始化\n    def begin_state(self, batch_size, device):\n        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens), device=device),\n                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens), device=device)]\n\n    def forward(self, X, *args):\n        # batch_size表*个句子，seq_len表每个句子*个单词，embedding_size表每个单词*维向量表示\n        X = self.embedding(X)  # embedding后shape:(batch_size, seq_len, embedding_size)\n        X = X.transpose(0, 1)  # 将第一和第二维度进行调换。 X shape:(seq_len, batch_size, embedding_size)\n        out, state = self.rnn(X)\n        # 输出的out包含每个隐藏状态 shape: (seq_len, batch_size, num_hiddens)\n        # state包含两个内容：最后一个时间步的隐层状态和记忆细胞 shape: (num_layers, batch_size, num_hiddens)\n    \n        return out, state\n        \n# 引入注意力机制的Seq2seq模型\nclass Seq2SeqAttentionDecoder(Decoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)\n        self.dense = nn.Linear(num_hiddens,vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_len, *args):\n        outputs, hidden_state = enc_outputs\n#         print(\"first:\",outputs.size(),hidden_state[0].size(),hidden_state[1].size())\n        # Transpose outputs to (batch_size, seq_len, hidden_size)\n        return (outputs.permute(1,0,-1), hidden_state, enc_valid_len)\n        #outputs.swapaxes(0, 1)\n        \n    def forward(self, X, state):\n        enc_outputs, hidden_state, enc_valid_len = state\n        #(\"X.size\",X.size())\n        X = self.embedding(X).transpose(0,1)\n#         print(\"Xembeding.size2\",X.size())\n        outputs = []\n        for l, x in enumerate(X):\n#             print(f\"\\n{l}-th token\")\n#             print(\"x.first.size()\",x.size())\n            # query shape: (batch_size, 1, hidden_size)\n            # select hidden state of the last rnn layer as query\n            query = hidden_state[0][-1].unsqueeze(1) # np.expand_dims(hidden_state[0][-1], axis=1)\n            # context has same shape as query\n#             print(\"query enc_outputs, enc_outputs:\\n\",query.size(), enc_outputs.size(), enc_outputs.size())\n            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)\n            # Concatenate on the feature dimension\n#             print(\"context.size:\",context.size())\n            x = torch.cat((context, x.unsqueeze(1)), dim=-1)\n            # Reshape x to (1, batch_size, embed_size+hidden_size)\n#             print(\"rnn\",x.size(), len(hidden_state))\n            out, hidden_state = self.rnn(x.transpose(0,1), hidden_state)\n            outputs.append(out)\n        outputs = self.dense(torch.cat(outputs, dim=0))\n        return outputs.transpose(0, 1), [enc_outputs, hidden_state,\n                                        enc_valid_len]","execution_count":12},{"metadata":{"id":"04D25163727B4DC7802D27098321E53F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,\n                            num_hiddens=16, num_layers=2)\n# encoder.initialize()\ndecoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8,\n                                  num_hiddens=16, num_layers=2)","execution_count":13},{"metadata":{"id":"770978FE74FB465EA35EE04B8597F5C7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"batch size=4\nseq_length=7\nhidden dim=16\nnum_layers=2\n\nencoder output size: torch.Size([7, 4, 16])\nencoder hidden size: torch.Size([2, 4, 16])\nencoder memory size: torch.Size([2, 4, 16])\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([2, 4, 16]))"},"transient":{},"execution_count":15}],"source":"# 查看效果\nX = torch.zeros((4, 7),dtype=torch.long)\nprint(\"batch size=4\\nseq_length=7\\nhidden dim=16\\nnum_layers=2\\n\")\nprint('encoder output size:', encoder(X)[0].size())\nprint('encoder hidden size:', encoder(X)[1][0].size())\nprint('encoder memory size:', encoder(X)[1][1].size())\nstate = decoder.init_state(encoder(X), None)\nout, state = decoder(X, state)\nout.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape","execution_count":15},{"metadata":{"id":"32E0F581A2B0454EBD69FC9CE0AA0AEC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 定义训练函数\nimport zipfile\nimport torch\nimport requests\nfrom io import BytesIO\nfrom torch.utils import data\nimport sys\nimport collections\n\nclass Vocab(object): # This class is saved in d2l.\n  def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n    # sort by frequency and token\n    counter = collections.Counter(tokens)\n    token_freqs = sorted(counter.items(), key=lambda x: x[0])\n    token_freqs.sort(key=lambda x: x[1], reverse=True)\n    if use_special_tokens:\n      # padding, begin of sentence, end of sentence, unknown\n      self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n      tokens = ['', '', '', '']\n    else:\n      self.unk = 0\n      tokens = ['']\n    tokens += [token for token, freq in token_freqs if freq >= min_freq]\n    self.idx_to_token = []\n    self.token_to_idx = dict()\n    for token in tokens:\n      self.idx_to_token.append(token)\n      self.token_to_idx[token] = len(self.idx_to_token) - 1\n      \n  def __len__(self):\n    return len(self.idx_to_token)\n  \n  def __getitem__(self, tokens):\n    if not isinstance(tokens, (list, tuple)):\n      return self.token_to_idx.get(tokens, self.unk)\n    else:\n      return [self.__getitem__(token) for token in tokens]\n    \n  def to_tokens(self, indices):\n    if not isinstance(indices, (list, tuple)):\n      return self.idx_to_token[indices]\n    else:\n      return [self.idx_to_token[index] for index in indices]\n\ndef load_data_nmt(batch_size, max_len, num_examples=1000):\n    \"\"\"Download an NMT dataset, return its vocabulary and data iterator.\"\"\"\n    # Download and preprocess\n    def preprocess_raw(text):\n        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n        out = ''\n        for i, char in enumerate(text.lower()):\n            if char in (',', '!', '.') and text[i-1] != ' ':\n                out += ' '\n            out += char\n        return out \n\n\n    with open('/home/kesci/input/fraeng6506/fra.txt', 'r') as f:\n      raw_text = f.read()\n\n\n    text = preprocess_raw(raw_text)\n\n    # Tokenize\n    source, target = [], []\n    for i, line in enumerate(text.split('\\n')):\n        if i >= num_examples:\n            break\n        parts = line.split('\\t')\n        if len(parts) >= 2:\n            source.append(parts[0].split(' '))\n            target.append(parts[1].split(' '))\n\n    # Build vocab\n    def build_vocab(tokens):\n        tokens = [token for line in tokens for token in line]\n        return Vocab(tokens, min_freq=3, use_special_tokens=True)\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n\n    # Convert to index arrays\n    def pad(line, max_len, padding_token):\n        if len(line) > max_len:\n            return line[:max_len]\n        return line + [padding_token] * (max_len - len(line))\n\n    def build_array(lines, vocab, max_len, is_source):\n        lines = [vocab[line] for line in lines]\n        if not is_source:\n            lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n        array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])\n        valid_len = (array != vocab.pad).sum(1)\n        return array, valid_len\n\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)\n    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)\n    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)\n    return src_vocab, tgt_vocab, train_iter\n\nembed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\nbatch_size, num_steps = 64, 10\nlr, num_epochs, ctx = 0.005, 500, False\n\nsrc_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)\nencoder = Seq2SeqEncoder(\n    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder = Seq2SeqAttentionDecoder(\n    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nmodel = EncoderDecoder(encoder, decoder)","execution_count":18},{"metadata":{"id":"62118E6BB9A04C77855FB6F632586C15","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'd2l'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-7ffb77ceb7e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/kesci/input/d2len9900'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_s2s_ch9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'd2l'"]}],"source":"import sys\nsys.path.append('/home/kesci/input/d2len9900')\nimport d2l\ntrain_s2s_ch9(model, train_iter, lr, num_epochs, ctx)","execution_count":21}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}