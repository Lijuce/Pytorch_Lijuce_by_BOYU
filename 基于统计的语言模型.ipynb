{"cells":[{"outputs":[{"output_type":"stream","text":"63282\n想要有直升机\n想要和你飞到宇宙去\n想要和你融化在一起\n融化在宇宙里\n我每天每天每\n","name":"stdout"}],"execution_count":2,"source":"# 读取数据\nwith open('/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt') as f:\n    corpus_chars = f.read()\nprint(len(corpus_chars))\nprint(corpus_chars[: 40])\ncorpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\ncorpus_chars = corpus_chars[: 10000]","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F66B84968F394D25ABDEE7A29B156012","scrolled":false}},{"metadata":{"id":"C84CF566EBF749FE8C880F658AA52AE5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1027\nchars: 想要有直升机 想要和你飞到宇宙去 想要和\nindices: [587, 251, 977, 765, 65, 121, 201, 587, 251, 811, 605, 64, 588, 981, 582, 114, 201, 587, 251, 811]\n","name":"stdout"}],"source":"# 建立字符索引\nidx_to_char = list(set(corpus_chars)) # 去重，得到索引到字符的映射\nchar_to_idx = {char: i for i, char in enumerate(idx_to_char)} # 字符到索引的映射\nvocab_size = len(char_to_idx)\nprint(vocab_size)\n\ncorpus_indices = [char_to_idx[char] for char in corpus_chars]  # 将每个字符转化为索引，得到一个索引的序列\nsample = corpus_indices[: 20]\nprint('chars:', ''.join([idx_to_char[idx] for idx in sample]))\nprint('indices:', sample)","execution_count":3},{"metadata":{"id":"57B4247520B642B38FBC40AF6FA3BFE8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 导入歌词数据函数\ndef load_data_jay_lyrics():\n    with open('/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt') as f:\n        corpus_chars = f.read()\n    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n    corpus_chars = corpus_chars[0:10000]\n    idx_to_char = list(set(corpus_chars))\n    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n    vocab_size = len(char_to_idx)\n    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n    return corpus_indices, char_to_idx, idx_to_char, vocab_size","execution_count":4},{"metadata":{"id":"61BCEBA90F2044B49233D18385E8AF63","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 为了给数据分批训练，需要将其分批次提取采样\n# 第一种：随机采样\nimport torch\nimport random\ndef data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符\n    num_examples = (len(corpus_indices) - 1) // num_steps  # 下取整，得到不重叠情况下的样本个数\n    example_indices = [i * num_steps for i in range(num_examples)]  # 每个样本的第一个字符在corpus_indices中的下标\n    random.shuffle(example_indices)\n\n    def _data(i):\n        # 返回从i开始的长为num_steps的序列\n        return corpus_indices[i: i + num_steps]\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    for i in range(0, num_examples, batch_size):\n        # 每次选出batch_size个随机样本\n        batch_indices = example_indices[i: i + batch_size]  # 当前batch的各个样本的首字符的下标\n        X = [_data(j) for j in batch_indices]\n        Y = [_data(j + 1) for j in batch_indices]\n        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)","execution_count":31},{"metadata":{"id":"371B074F2456405FA34261A9B10AA291","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"original_seq:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nx:  tensor([[10, 11, 12, 13, 14],\n        [ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9]])\ny:  tensor([[11, 12, 13, 14, 15],\n        [ 1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10]])\n","name":"stdout"}],"source":"# 随机采样的效果\ntest_seq = list(range(20))\nprint('original_seq: ', test_seq)\nfor x, y in data_iter_random(test_seq, 4, 5):\n    print('x: ', x)\n    print('y: ', y)","execution_count":32},{"metadata":{"id":"E6F1E64ABB6C4B22851C80E7BE3DC755","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 第二种：相邻采样\ndef data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    corpus_len = len(corpus_indices) // batch_size * batch_size  # 保留下来的序列的长度\n    corpus_indices = corpus_indices[: corpus_len]  # 仅保留前corpus_len个字符\n    indices = torch.tensor(corpus_indices, device=device)\n    indices = indices.view(batch_size, -1)  # resize成(batch_size, )\n    batch_num = (indices.shape[1] - 1) // num_steps\n    for i in range(batch_num):\n        i = i * num_steps\n        X = indices[:, i: i + num_steps]\n        Y = indices[:, i + 1: i + num_steps + 1]\n        yield X, Y","execution_count":33},{"metadata":{"id":"45B6ACFF081D454D8AD5BEB3542FFE62","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"X:  tensor([[ 0,  1,  2,  3,  4,  5],\n        [10, 11, 12, 13, 14, 15]]) \nY: tensor([[ 1,  2,  3,  4,  5,  6],\n        [11, 12, 13, 14, 15, 16]]) \n\n","name":"stdout"}],"source":"# 相邻采样效果\nfor X, Y in data_iter_consecutive(test_seq, batch_size=2, num_steps=6):\n    print('X: ', X, '\\nY:', Y, '\\n')","execution_count":35},{"metadata":{"id":"32B22B2B72CB4AEFB2E4E8D7F5E429E7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}