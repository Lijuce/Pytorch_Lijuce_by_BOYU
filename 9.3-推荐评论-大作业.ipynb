{"cells":[{"outputs":[],"execution_count":1,"source":"!pip install torchtext\nimport collections\nimport os\nimport random\nimport time\nimport torch\nfrom torch import nn\nimport torchtext.vocab as Vocab\nimport torch.utils.data as Data\nimport torch.nn.functional as F\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3F3D6A9CCF10407F8878735B77420968","scrolled":false}},{"metadata":{"id":"FFEC9FB8F86D40278C92D0C6DDCBFFE6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import csv\ndata = []\nwith open('/home/kesci/input/Comments9120/train_shuffle.txt') as f:\n    text = f.read().split(\"\\n\")\n    for t in text:\n        t_ = t.split(\"\\t\")\n        data.append(t_)\nlabels_data = []\nreviews_data = []\nfor d in data:\n    # for label, review in d:\n    label = d[0]\n    review = d[-1]\n    labels_data.append(label)\n    reviews_data.append(review)\ndata = (reviews_data[:-1], labels_data[:-1])  # 最后一个为空\ndata_test = (reviews_data[:10], labels_data[:10])\ndata_test","execution_count":2},{"metadata":{"id":"468421F279FB4B95897BA94FD3E7DDEB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"2509"},"transient":{},"execution_count":3}],"source":"def get_tokenized_imdb(data):\n    '''\n    @params:\n        data: 数据的列表，列表中的每个元素为 [文本字符串，0/1标签] 二元组\n    @return: 切分词后的文本的列表，列表中的每个元素为切分后的词序列\n    '''\n    review_list = []\n    for reviews in data[0]:\n        tok_list = []\n        for tok in reviews:\n            tok_list.append(tok)\n        review_list.append(tok_list) \n    return review_list\n\ndef get_vocab_imdb(data):\n    '''\n    @params:\n        data: 同上\n    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n    '''\n    tokenized_data = get_tokenized_imdb(data)\n    counter = collections.Counter([tok for token in tokenized_data for tok in token])\n    return Vocab.Vocab(counter, min_freq=1)\n\n# vocab = get_tokenized_imdb(data)\nvocab = get_vocab_imdb(data)\nlen(vocab)\n# vo_test = Vocab.Vocab(collections.Counter([tok for token in get_tokenized_imdb(data_test) for tok in token]), min_freq=1)\n# len(vo_test)","execution_count":3},{"metadata":{"id":"99BF3C8A9D5E49E0A4987F967C56E05E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def preprocess_imdb(data, vocab):\n    '''\n    @params:\n        data: 同上，原始的读入数据\n        vocab: 训练集上生成的词典\n    @return:\n        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n        labels: 情感标签，形状为 (n,) 的0/1整数张量\n    '''\n    max_l = 20  # 将每条评论通过截断或者补0，使得长度变成20---因最大长度即为20\n\n    def pad(x):\n        if len(x) > max_l:\n            return x[:max_l]\n        else:\n            return x + [0] * (max_l - len(x))\n    tokenized_data = get_tokenized_imdb(data)\n    # TEXT.vocab.stoi 对应词寻找下标； itos 对应下标寻找词 \n    features = torch.tensor([pad([vocab.stoi[word] for word in words]) \n                            for words in tokenized_data])\n    labels = torch.tensor([int(sco) for _, score in [data] for sco in score ])\n    return features, labels","execution_count":4},{"metadata":{"id":"905D126801BE478C8C6E0BE47E7650DD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(tensor([[ 102,   19,   28,  ...,    0,    0,    0],\n         [ 500,  176,  434,  ...,    0,    0,    0],\n         [ 248,  492,  124,  ...,    0,    0,    0],\n         ...,\n         [  64,   25,   46,  ...,    0,    0,    0],\n         [  35,   45,  144,  ...,    0,    0,    0],\n         [  85, 1438,   78,  ...,    0,    0,    0]]),\n tensor([0, 0, 0,  ..., 0, 1, 0]))"},"transient":{},"execution_count":5}],"source":"# preprocess_imdb(data_test, vocab)\ntrain_fea, train_lab = preprocess_imdb(data, vocab)\n# get_tokenized_imdb(data)\ntrain_fea, train_lab","execution_count":5},{"metadata":{"id":"87F45252216148BD98B99A1C6C39FE47","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"<torch.utils.data.dataset.TensorDataset at 0x7f87454035f8>"},"transient":{},"execution_count":6}],"source":"train_set = Data.TensorDataset(train_fea, train_lab)\ntrain_set","execution_count":6},{"metadata":{"id":"9EBA2F53005849BC8AB4CFEC9CA2A3AA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class BiRNN(nn.Module):\n    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n        '''\n        @params:\n            vocab: 在数据集上创建的词典，用于获取词典大小\n            embed_size: 嵌入维度大小\n            num_hiddens: 隐藏状态维度大小\n            num_layers: 隐藏层个数\n        '''\n        super(BiRNN, self).__init__()\n        self.embedding = nn.Embedding(len(vocab), embed_size)\n        \n        # encoder-decoder framework\n        # bidirectional设为True即得到双向循环神经网络\n        self.encoder = nn.LSTM(input_size=embed_size, \n                                hidden_size=num_hiddens, \n                                num_layers=num_layers,\n                                bidirectional=True)\n        self.decoder = nn.Linear(4*num_hiddens, 2) # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n    \n    def forward(self, inputs):\n        '''\n        @params:\n            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n        @return:\n            outs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n        '''\n        # 因为LSTM需要将序列长度(seq_len)作为第一维，所以需要将输入转置\n        embeddings = self.embedding(inputs.permute(1, 0)) # (seq_len, batch_size, d)\n        # rnn.LSTM 返回输出、隐藏状态和记忆单元，格式如 outputs, (h, c)\n        outputs, _ = self.encoder(embeddings) # (seq_len, batch_size, 2*h)\n        encoding = torch.cat((outputs[0], outputs[-1]), -1) # (batch_size, 4*h)\n        outs = self.decoder(encoding) # (batch_size, 2)\n        return outs\n\n\n# for X, y in train_iter:\n#     print('X', X.shape, 'y', y.shape)\n#     break\n# print('#batches:', len(train_iter))\nvocab = get_vocab_imdb(data)\nbatch_size = 64\ntrain_fea, train_lab = preprocess_imdb(data, vocab)\ntrain_set = Data.TensorDataset(train_fea, train_lab)\ntrain_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\nembed_size, num_hiddens, num_layers = 100, 100, 4\nnet = BiRNN(vocab, embed_size, num_hiddens, num_layers)","execution_count":8},{"metadata":{"id":"E92BF3888DB949EF825C40C6574CDE6D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"cache_dir = \"/home/kesci/work/Comments9120\"\nglove_vocab = Vocab.Vectors(name='wiki_100.utf8', cache=cache_dir)","execution_count":9},{"metadata":{"id":"7EC777E62144451797AA6785D76E08B7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"There are 4 oov words.\n","name":"stdout"}],"source":"def load_pretrained_embedding(words, pretrained_vocab):\n    '''\n    @params:\n        words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n        pretrained_vocab: 预训练词向量\n    @return:\n        embed: 加载到的词向量\n    '''\n    embed= torch.zeros(len(words), 100)  # 初始化语料对应词向量\n    oov_count = 0  # 计总袋外词数量\n    for i, word in enumerate(words):\n        try:\n            idx = pretrained_vocab.stoi[word]\n            embed[i, :] = pretrained_vocab.vectors[idx]\n        except KeyError:\n            oov_count += 1\n    \n    if oov_count > 0:\n        print(\"There are %d oov words.\" % oov_count)\n    return embed\n    \nnet.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))\nnet.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它","execution_count":10},{"metadata":{"id":"79F17C6D2558432085C38858B67DCF72","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def evaluate_accuracy(data_iter, net, device=None):\n    if device is None and isinstance(net, torch.nn.Module):\n        device = list(net.parameters())[0].device \n    acc_sum, n = 0.0, 0\n    with torch.no_grad():\n        for X, y in data_iter:\n            if isinstance(net, torch.nn.Module):\n                net.eval()\n                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n                net.train()\n            else:\n                if('is_training' in net.__code__.co_varnames):\n                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n                else:\n                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n            n += y.shape[0]\n    return acc_sum / n\n\ndef evaluate_test_set(data_iter, net, device=None):\n    if device is None and isinstance(net, torch.nn.Module):\n        device = list(net.parameters())[0].device \n    # acc_sum, n = 0.0, 0\n    with torch.no_grad():\n        for X, _ in data_iter:\n            if isinstance(net, torch.nn.Module):\n                net.eval()\n                test_result = net(X.to(device))\n                net.train()\n                print(test_result)\n            # else:\n            #     if('is_training' in net.__code__.co_varnames):\n            #         acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n            #     else:\n            #         acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n            # n += y.shape[0]\n    # return acc_sum / n\n    \ndef train(train_iter, net, loss, optimizer, device, num_epochs):\n    net = net.to(device)\n    print(\"training on \", device)\n    batch_count = 0\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n        for X, y in train_iter:\n            X = X.to(device)\n            y = y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y) \n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            train_l_sum += l.cpu().item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n            n += y.shape[0]\n            batch_count += 1\n        # test_acc = evaluate_accuracy(test_iter, net)\n        print('epoch %d, loss %.4f, train acc %.3f, time %.1f sec'\n              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, time.time() - start))","execution_count":11},{"metadata":{"id":"D7F7A5ADBDED4ABEAE1469A28F96D62D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"training on  cuda\nepoch 1, loss 0.3690, train acc 0.839, time 3.2 sec\nepoch 2, loss 0.1273, train acc 0.900, time 3.0 sec\nepoch 3, loss 0.0718, train acc 0.918, time 3.0 sec\nepoch 4, loss 0.0463, train acc 0.928, time 3.1 sec\nepoch 5, loss 0.0315, train acc 0.940, time 3.1 sec\nepoch 6, loss 0.0224, train acc 0.948, time 3.1 sec\nepoch 7, loss 0.0155, train acc 0.958, time 3.0 sec\nepoch 8, loss 0.0111, train acc 0.966, time 3.1 sec\nepoch 9, loss 0.0077, train acc 0.974, time 3.0 sec\nepoch 10, loss 0.0067, train acc 0.974, time 3.0 sec\n","name":"stdout"}],"source":"# vocab = get_vocab_imdb(data)  # 获得语料词典库\n\n# train_fea, train_lab = preprocess_imdb(data, vocab)\n# train_set = Data.TensorDataset(train_fea, train_lab)\n# batch_size = 64\n# train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\nembed_size, num_hiddens, num_layers = 100, 100, 2\nnet = BiRNN(vocab, embed_size, num_hiddens, num_layers)\n\nlr, num_epochs = 0.001, 10\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\nloss = nn.CrossEntropyLoss()\n\ntrain(train_iter, net, loss, optimizer, device, num_epochs)","execution_count":25},{"metadata":{"id":"B0431942EF6349F18E5564665B76E285","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"test_data_list = []\nwith open('/home/kesci/input/Comments9120/test_handout.txt') as f:\n    test_data = f.read().split(\"\\n\")\n    test_data_list.append(test_data)\n    test_data_list.append(['0'] * len(test_data))\n    # print(test_data_list)","execution_count":13},{"metadata":{"id":"656B582E6AC5470D85DCB46FB9B6E4A0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"<torch.utils.data.dataset.TensorDataset at 0x7f86d0055710>"},"transient":{},"execution_count":26}],"source":"test_fea, test_lab = preprocess_imdb(test_data_list, vocab)\ntest_set = Data.TensorDataset(test_fea, test_lab)\ntest_iter = Data.DataLoader(test_set, batch_size)\ntest_set","execution_count":26},{"metadata":{"id":"7C4BBAE9FC0945C192CF2871006D6E3C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def evaluate_test_set(data_iter, net, device=None):\n    # net = net.to(device)\n    if device is None and isinstance(net, torch.nn.Module):\n        device = list(net.parameters())[0].device \n    # device = list(net.parameters())[0].device \n    # acc_sum, n = 0.0, 0\n    result = []\n    with torch.no_grad():\n        for X, _ in data_iter:\n            if isinstance(net, torch.nn.Module):\n                net.eval()\n                test_result = net(X.to(device)).softmax(dim=1)\n                result.append(test_result)\n        return result\n                \nre = evaluate_test_set(test_iter, net, device)","execution_count":27},{"metadata":{"id":"3EF3791FD18A459A843C2CEA9EC25891","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"score_l = []\nfor r in re:\n    r_view = r.cpu().numpy()\n    for score in r_view:\n        score_l.append(score[-1])\n\nimport pandas as pd\nd_test = pd.read_csv(\"/home/kesci/input/Comments9120/test_handout.txt\", header=None)\nd_test.insert(0,\"Prediction\", score_l[:-1])\noutput = d_test.drop(columns=0,axis=1)  # 移除评论内容一列\noutput.to_csv(\"result.csv\")","execution_count":29},{"metadata":{"id":"C4D8B1157FC547A58D7352ABF9B6BF7F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":30}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}