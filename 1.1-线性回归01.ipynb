{"cells":[{"outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"execution_count":1,"source":"%matplotlib inline\nimport torch\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport random\n\nprint(torch.__version__)","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A229A533D6B6401A865F1103D0BDBBFE","scrolled":false}},{"metadata":{"id":"83BD5C97C9C641008A56FACAB21BE77A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# set input feature number \nnum_inputs = 2\n# set example number\nnum_examples = 1000\n\n# set true weight and bias in order to generate corresponded label\ntrue_w = [2, -3.4]\ntrue_b = 4.2\n\nfeatures = torch.randn(num_examples, num_inputs,\n                      dtype=torch.float32)\nlabels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),\n                       dtype=torch.float32)","execution_count":2},{"metadata":{"id":"603589FCA1044B438FADC10D87E2F4CB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/603589FCA1044B438FADC10D87E2F4CB/q5izcte0oi.png\">"},"transient":{}}],"source":"plt.scatter(features[:, 1].numpy(), labels.numpy(), 1);","execution_count":3},{"metadata":{"id":"00F2AE4AFF6C4B8C98EE1126DE6B2F00","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)  # random read 10 samples\n    for i in range(0, num_examples, batch_size):\n        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # the last time may be not enough for a whole batch\n        yield  features.index_select(0, j), labels.index_select(0, j)","execution_count":4},{"metadata":{"id":"7B7C57D58A744F35A007008CAB34EFEC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[-3.1768, -0.1232],\n        [-0.6662, -0.0613],\n        [ 0.1475, -0.6895],\n        [ 0.5243, -0.5755],\n        [-0.9027,  0.5149],\n        [-0.4269,  0.4762],\n        [-0.4361, -1.2426],\n        [ 0.0554, -0.4465],\n        [-0.3154,  0.5607],\n        [ 1.2315,  0.1099]]) \n tensor([-1.7161,  3.0805,  6.8387,  7.2061,  0.6532,  1.7397,  7.5588,  5.8264,\n         1.6551,  6.2811])\n","name":"stdout"}],"source":"batch_size = 10\n\nfor X, y in data_iter(batch_size, features, labels):\n    print(X, '\\n', y)\n    break","execution_count":15},{"metadata":{"id":"C3B6DDE304C0464983503751B2079307","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([0.], requires_grad=True)"},"transient":{},"execution_count":6}],"source":"w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32)\nb = torch.zeros(1, dtype=torch.float32)\n\nw.requires_grad_(requires_grad=True)\nb.requires_grad_(requires_grad=True)","execution_count":6},{"metadata":{"id":"0DFB6074636C4DA987E2F174B7077EDD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def linreg(X, w, b):\n    return torch.mm(X, w) + b\n\ndef squared_loss(y_hat, y): \n    return (y_hat - y.view(y_hat.size())) ** 2 / 2\n    \ndef sgd(params, lr, batch_size): \n    for param in params:\n        param.data -= lr * param.grad / batch_size","execution_count":7},{"metadata":{"id":"25FA84A8C0E846D08E41A0BD0685F94F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 1, loss 0.030283\nepoch 2, loss 0.000102\nepoch 3, loss 0.000047\nepoch 4, loss 0.000047\nepoch 5, loss 0.000047\n","name":"stdout"}],"source":"lr = 0.03\nnum_epochs = 5\n\nnet = linreg\nloss = squared_loss\n\n# training\nfor epoch in range(num_epochs):  # training repeats num_epochs times\n    # in each epoch, all the samples in dataset will be used once\n    \n    # X is the feature and y is the label of a batch sample\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y).sum()  \n        # calculate the gradient of batch sample loss \n        l.backward()  \n        # using small batch random gradient descent to iter model parameters\n        sgd([w, b], lr, batch_size)  \n        # reset parameter gradient\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n    train_l = loss(net(features, w, b), labels)\n    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))","execution_count":8},{"metadata":{"id":"A8AD050FB492420789A24378A7CBEB37","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"for x,y in data_iter(10, features, labels):\n    x_test = x\n    y_test = y","execution_count":16},{"metadata":{"id":"01D99967FE2B4533A60B2E2080828752","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[-0.8610,  0.4798],\n        [-1.1526,  0.6640],\n        [ 0.4182,  1.9204],\n        [ 0.8108, -2.0770],\n        [-1.5355, -1.4060],\n        [-1.4790, -1.0699],\n        [ 0.9932,  1.0587],\n        [-1.3455, -0.6910],\n        [-0.3478, -0.8518],\n        [-0.0420, -1.4659]])\ntensor([-0.8610,  0.4798, -1.1526,  0.6640,  0.4182,  1.9204,  0.8108, -2.0770,\n        -1.5355, -1.4060, -1.4790, -1.0699,  0.9932,  1.0587, -1.3455, -0.6910,\n        -0.3478, -0.8518, -0.0420, -1.4659])\n","name":"stdout"}],"source":"print(x_test)\nx_view = x_test.view(-1)\nprint(x_view)","execution_count":31},{"metadata":{"id":"F3FD6ED56B9147758BE99A7E0B49B360","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"torch.Size([20, 1])"},"transient":{},"execution_count":43}],"source":"# x_view.view(x_test.size())\n# x_view.size()\nx_view.view(-1,1).shape","execution_count":43}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}