{"cells":[{"outputs":[],"execution_count":1,"source":"import collections\nimport math\nimport random\nimport sys\nimport time\nimport os\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.utils.data as Data","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5D2C86D6A7F4426887B388960E9B68DC","scrolled":false}},{"metadata":{"id":"CF0C432E34DC4D4A96F24A4607D47E99","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"# tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter']\n# tokens: 15 ['pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N']\n# tokens: 11 ['mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group']\n# tokens: 23 ['rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate']\n# tokens: 34 ['a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of', 'workers', 'exposed', 'to', 'it', 'more', 'than', 'N', 'years', 'ago', 'researchers', 'reported']\n","name":"stdout"}],"source":"with open('/home/kesci/input/ptb_train1020/ptb.train.txt', 'r') as f:\n    lines = f.readlines() # 该数据集中句子以换行符为分割符进行切割\n    raw_dataset = [st.split() for st in lines]\n    \n# 对于数据集的前3个句子，打印每个句子的词数和前5个词\n# 句尾符为 '<EOS>' ，生僻词全用 '<unk>' 表示，数字则被替换成了 'N'\nfor st in raw_dataset[:5]:\n    print('# tokens:', len(st), st)","execution_count":11},{"metadata":{"id":"4A080C9192424C7283746E1C53134CA9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'# tokens: 887100'"},"transient":{},"execution_count":12}],"source":"# 构建词-索引\ncounter = collections.Counter([tk for st in raw_dataset for tk in st])\ncounter = dict(filter(lambda x: x[1] >= 5, counter.items())) # 只保留在数据集中至少出现5次的词\n\nidx_to_token = [tk for tk, _ in counter.items()]\ntoken_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n\ndataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n           for st in raw_dataset] # raw_dataset中的单词在这一步被转换为对应的idx\n# for st in raw_dataset:\n#     for tk in st:\n#         if tk in token_to_idx:\n#             li = token_to_idx[tk]\nnum_tokens = sum([len(st) for st in dataset])\n'# tokens: %d' % num_tokens","execution_count":12},{"metadata":{"id":"753A3E7A08654CA9B01B7B9ABEE8E787","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"# tokens: 375537\n# the: before=50770, after=2156\n# join: before=45, after=45\n","name":"stdout"}],"source":"# 二次采样的需要：高频无意义词（如the、a等）对模型的训练益处不大\n# 因此需要二次采样过程即将这些词以一定概率进行丢弃\n\n# 实现概率丢弃的函数\ndef discard(idx):\n    '''\n    @params:\n        idx: 单词的下标\n    @return: True/False 表示是否丢弃该单词\n    '''\n    return random.uniform(0, 1) < 1 - math.sqrt(\n        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n\n# 丢弃后的效果\nsubsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\nprint('# tokens: %d' % sum([len(st) for st in subsampled_dataset]))\n\ndef compare_counts(token):\n    return '# %s: before=%d, after=%d' % (token, sum(\n        [st.count(token_to_idx[token]) for st in dataset]), sum(\n        [st.count(token_to_idx[token]) for st in subsampled_dataset]))\n\nprint(compare_counts('the'))\nprint(compare_counts('join'))","execution_count":13},{"metadata":{"id":"57A54082329340028F30218820ACD669","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 训练跳字模型前，需先分别获得中心词和背景词\ndef get_centers_and_contexts(dataset, max_window_size):\n    '''\n    @params:\n        dataset: 数据集为句子的集合，每个句子则为单词的集合，此时单词已经被转换为相应数字下标\n        max_window_size: 背景词的词窗大小的最大值\n    @return:\n        centers: 中心词的集合\n        contexts: 背景词窗的集合，与中心词对应，每个背景词窗则为背景词的集合\n    '''\n    centers, contexts = [], []\n    for st in dataset:\n        if len(st) < 2:  # 每个句子至少要有2个词才可能组成一对“中心词-背景词”\n            continue\n        centers += st\n        for center_i in range(len(st)):\n            window_size = random.randint(1, max_window_size) # 随机选取背景词窗大小\n            indices = list(range(max(0, center_i - window_size),\n                                 min(len(st), center_i + 1 + window_size)))\n            indices.remove(center_i)  # 将中心词排除在背景词之外\n            contexts.append([st[idx] for idx in indices])\n    return centers, contexts\n\nall_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)\n","execution_count":14},{"metadata":{"id":"F90AFCA72DEE467D884835F0D9A1740D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\ncenter 0 has contexts [1, 2]\ncenter 1 has contexts [0, 2]\ncenter 2 has contexts [1, 3]\ncenter 3 has contexts [1, 2, 4, 5]\ncenter 4 has contexts [3, 5]\ncenter 5 has contexts [4, 6]\ncenter 6 has contexts [4, 5]\ncenter 7 has contexts [8]\ncenter 8 has contexts [7, 9]\ncenter 9 has contexts [7, 8]\n","name":"stdout"}],"source":"# 查看效果\ntiny_dataset = [list(range(7)), list(range(7, 10))]\nprint('dataset', tiny_dataset)\nfor center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n    print('center', center, 'has contexts', context)","execution_count":15},{"metadata":{"id":"CA074327C95C44FB846B1B7A4CB11B0D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Parameter containing:\ntensor([[-1.9990, -0.8441, -0.3697,  1.0407],\n        [ 1.5786,  0.2992,  1.0993,  0.2073],\n        [-1.0477, -0.1431,  0.9992,  1.6874],\n        [-0.1563, -0.4959, -1.0224, -0.8207],\n        [ 0.5232,  1.3195, -0.9368,  0.6083],\n        [ 1.7812, -0.6878, -0.9899,  0.2132],\n        [ 0.4221, -0.1444, -0.7666,  1.2375],\n        [-1.0436, -0.1406, -0.0798, -0.5636],\n        [-0.5455,  0.7346, -1.6059,  0.2228],\n        [ 1.4952,  0.3803, -0.4006, -0.3831]], requires_grad=True)\ntensor([[[ 1.5786,  0.2992,  1.0993,  0.2073],\n         [-1.0477, -0.1431,  0.9992,  1.6874],\n         [-0.1563, -0.4959, -1.0224, -0.8207]],\n\n        [[ 0.5232,  1.3195, -0.9368,  0.6083],\n         [ 1.7812, -0.6878, -0.9899,  0.2132],\n         [ 0.4221, -0.1444, -0.7666,  1.2375]]], grad_fn=<EmbeddingBackward>)\n","name":"stdout"}],"source":"# 使用pytorch内置Embedding方法\nembed = nn.Embedding(num_embeddings=10, embedding_dim=4)  # 定义初始化\nprint(embed.weight)\n\n# 查看效果\nx = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)\nprint(embed(x))","execution_count":16},{"metadata":{"id":"83893F825B964736826051F88D667BD6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"torch.Size([2, 1, 6])\n","name":"stdout"}],"source":"# 由于训练过程需要用到批量乘法，因此先简单了解PyTorch 预置的批量乘法\nX = torch.ones((2, 1, 4))\nY = torch.ones((2, 4, 6))\nprint(torch.bmm(X, Y).shape)","execution_count":17},{"metadata":{"id":"EBEB68595E2D40B38CAC0A0B30127A72","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# Skip-Gram 模型的前向计算\ndef skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n    '''\n    @params:\n        center: 中心词下标，形状为 (n, 1) 的整数张量\n        contexts_and_negatives: 背景词和噪音词下标，形状为 (n, m) 的整数张量\n        embed_v: 中心词的 embedding 层\n        embed_u: 背景词的 embedding 层\n    @return:\n        pred: 中心词与背景词（或噪音词）的内积，之后可用于计算概率 p(w_o|w_c)\n    '''\n    # n 整个词典大小； d每个词向量维度大小\n    v = embed_v(center) # 中心词的shape (n, 1, d)\n    u = embed_u(contexts_and_negatives) # shape of (n, m, d)\n    pred = torch.bmm(v, u.permute(0, 2, 1)) # bmm((n, 1, d), (n, d, m)) => shape of (n, 1, m)\n    return pred","execution_count":20},{"metadata":{"id":"578C064D7A784F50A58D2BDF6DB5CCB3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 负采样操作\ndef get_negatives(all_contexts, sampling_weights, K):\n    '''\n    @params:\n        all_contexts: [[w_o1, w_o2, ...], [...], ... ]\n        sampling_weights: 每个单词的噪声词采样概率\n        K: 随机采样个数\n    @return:\n        all_negatives: [[w_n1, w_n2, ...], [...], ...]\n    '''\n    all_negatives, neg_candidates, i = [], [], 0\n    population = list(range(len(sampling_weights)))\n    for contexts in all_contexts:\n        negatives = []\n        while len(negatives) < len(contexts) * K:\n            if i == len(neg_candidates):\n                # 根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词。\n                # 为了高效计算，可以将k设得稍大一点\n                i, neg_candidates = 0, random.choices(\n                    population, sampling_weights, k=int(1e5))\n            neg, i = neg_candidates[i], i + 1\n            # 噪声词不能是背景词\n            if neg not in set(contexts):\n                negatives.append(neg)\n        all_negatives.append(negatives)\n    return all_negatives","execution_count":21},{"metadata":{"id":"FB00A75EAF8547DD8FF3A4732945BE43","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 查看负采样的效果\nsampling_weights = [counter[w]**0.75 for w in idx_to_token]\nall_negatives = get_negatives(all_contexts, sampling_weights, 5)","execution_count":22},{"metadata":{"id":"D6F8BB7EA3534F938E0DE1BD17C63A80","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"centers shape: torch.Size([512, 1])\ncontexts_negatives shape: torch.Size([512, 60])\nmasks shape: torch.Size([512, 60])\nlabels shape: torch.Size([512, 60])\n","name":"stdout"}],"source":"# 批量读取数据\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, centers, contexts, negatives):\n        assert len(centers) == len(contexts) == len(negatives)\n        self.centers = centers\n        self.contexts = contexts\n        self.negatives = negatives\n        \n    def __getitem__(self, index):\n        return (self.centers[index], self.contexts[index], self.negatives[index])\n\n    def __len__(self):\n        return len(self.centers)\n\ndef batchify(data):\n    '''\n    用作DataLoader的参数collate_fn\n    @params:\n        data: 长为batch_size的列表，列表中的每个元素都是__getitem__得到的结果\n    @outputs:\n        batch: 批量化后得到 (centers, contexts_negatives, masks, labels) 元组\n            centers: 中心词下标，形状为 (n, 1) 的整数张量\n            contexts_negatives: 背景词和噪声词的下标，形状为 (n, m) 的整数张量\n            masks: 与补齐相对应的掩码，形状为 (n, m) 的0/1整数张量\n            labels: 指示中心词的标签，形状为 (n, m) 的0/1整数张量\n    '''\n    max_len = max(len(c) + len(n) for _, c, n in data)  # 背景词+噪声词的总最大长度\n    centers, contexts_negatives, masks, labels = [], [], [], []\n    for center, context, negative in data:\n        cur_len = len(context) + len(negative)  # 当前背景词+噪声词总长度\n        centers += [center]\n        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]  # 补齐元素\n        masks += [[1] * cur_len + [0] * (max_len - cur_len)] # 使用掩码变量mask来避免填充项对损失函数计算的影响\n        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n        batch = (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n            torch.tensor(masks), torch.tensor(labels))\n    return batch\n\nbatch_size = 512\nnum_workers = 0 if sys.platform.startswith('win32') else 4\n\n# 查看下效果\ndataset = MyDataset(all_centers, all_contexts, all_negatives)\ndata_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n                            collate_fn=batchify, \n                            num_workers=num_workers)\n\nfor batch in data_iter:\n    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n                           'labels'], batch):\n        print(name, 'shape:', data.shape)\n        # print(data)\n    break","execution_count":31},{"metadata":{"id":"78D823C933994DF28FFEABAB3CE3574B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([0.8740, 1.2100])\n","name":"stdout"}],"source":"# 损失函数。使用二元交叉熵损失函数进行计算\nclass SigmoidBinaryCrossEntropyLoss(nn.Module):\n    def __init__(self):\n        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n    def forward(self, inputs, targets, mask=None):\n        '''\n        @params:\n            inputs: 经过sigmoid层后为预测D=1的概率\n            targets: 0/1向量，1代表背景词，0代表噪音词\n        @return:\n            res: 平均到每个label的loss\n        '''\n        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n        res = res.sum(dim=1) / mask.float().sum(dim=1)\n        return res\n\nloss = SigmoidBinaryCrossEntropyLoss()\n\n# 查看效果\npred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\nlabel = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]]) # 标签变量label中的1和0分别代表背景词和噪声词\nmask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])  # 掩码变量\nprint(loss(pred, label, mask))","execution_count":34},{"metadata":{"id":"32520666EC6648A6B30C4D2394B5541A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 模型构建。两个嵌入层，对应中心词和背景词\nembed_size = 100\nnet = nn.Sequential(nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n                    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size))","execution_count":36},{"metadata":{"id":"E9A6613E8E134B7BBCE6B6C3B119AEF4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"train on cpu\n","name":"stdout"}],"source":"# 训练模型\ndef train(net, lr, num_epochs):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(\"train on\", device)\n    net = net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    for epoch in range(num_epochs):\n        start, l_sum, n = time.time(), 0.0, 0\n        for batch in data_iter:\n            center, context_negative, mask, label = [d.to(device) for d in batch]\n            \n            pred = skip_gram(center, context_negative, net[0], net[1])\n            \n            l = loss(pred.view(label.shape), label, mask).mean() # 一个batch的平均loss\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            l_sum += l.cpu().item()\n            n += 1\n        print('epoch %d, loss %.2f, time %.2fs'\n              % (epoch + 1, l_sum / n, time.time() - start))\n\ntrain(net, 0.01, 5)","execution_count":null},{"metadata":{"id":"A138B7D88D464C858D5676FC0519B5A4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 测试模型\ndef get_similar_tokens(query_token, k, embed):\n    '''\n    @params:\n        query_token: 给定的词语\n        k: 近义词的个数\n        embed: 预训练词向量\n    '''\n    W = embed.weight.data\n    x = W[token_to_idx[query_token]]\n    # 添加的1e-9是为了数值稳定性\n    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n    _, topk = torch.topk(cos, k=k+1)\n    topk = topk.cpu().numpy()\n    for i in topk[1:]:  # 除去输入词\n        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n        \nget_similar_tokens('chip', 3, net[0])","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}